# You run this with
# python3 -m mastml.mastml.driver inputfile.conf inputdata.csv -o outputdirectory

[GeneralSetup]
    input_features = Auto
    target_feature = Reduced barrier (eV)
    randomizer = False
    metrics = Auto
    not_input_features = Host element, Solute element, predict_Pt, predict_Ag, predict_W
    #not_input_features = predict_Pt, predict_Ag, predict_W
    grouping_feature = Host element
    validation_columns = predict_Ag
    #validation_columns = predict_Pt, predict_Ag, predict_W

[DataCleaning]
    cleaning_method = remove
    imputation_strategy = mean

#[Clustering]
#    [[KMeans_5Clusters]]
#        n_clusters = 5
#    [[SpectralClustering_14Clusters]]
#        n_clusters = 14

[FeatureGeneration]
    [[Magpie]]
        composition_feature = Host element
    #[[ContainsElement]]
    #    composition_feature = Host element
    #    all_elements = True
    #    element = Al # Ignored if all_elements = True
    #    new_name = has_Al # Ignored if all_elements = True

[FeatureNormalization]
    [[StandardScaler]]

#[LearningCurve]
#    estimator = KernelRidge_select
#    cv = RepeatedKFold_select
#    scoring = root_mean_squared_error
#    n_features_to_select = 20
#    selector_name = MASTMLFeatureSelector

[FeatureSelection]
    [[RFE]]
        estimator = MLPRegressor
        n_features_to_select = 20
    #[[SequentialFeatureSelector]]
    #     estimator = RandomForestRegressor
    #     k_features = 10
    #[[RFECV]]
    #     estimator = RandomForestRegressor_select
    #     step = 1
    #     cv = LeaveOneGroupOut_select
    #     #cv = RepeatedKFold_select
    #     #cv = KFold_select
    #[[MASTMLFeatureSelector]]
    #    estimator = KernelRidge_select
    #    n_features_to_select = 5
    #    #cv = KFold_select
    #    cv = LeaveOneGroupOut_select
    #[[SelectKBest]]

[DataSplits]
    #[[NoSplit]]
    #[[KFold_select]]
    #    shuffle = True
    #    n_splits = 10
    #[[RepeatedKFold_select]]
    #    n_splits = 5
    #    n_repeats = 2
    [[RepeatedKFold]]
        n_splits = 3
        n_repeats = 2
    #[[LeaveOneGroupOut_select]]
    #    grouping_column = Host element
    #[[LeaveOneGroupOut_learning]]
    #    grouping_column = Host element
    #[[LeaveOneGroupOut_Al]]
    #    grouping_column = has_Al
    #[[LeaveOneGroupOut_Zr]]
    #    grouping_column = has_Zr
    #[[LeaveOneGroupOut_Mo]]
    #    grouping_column = has_Mo
    #[[LeaveOneGroupOut_host]]
    #    grouping_column = Host element
    #[[LeaveOneGroupOut_kmeans]]
    #    grouping_column = KMeans_5Clusters
    #[[LeaveOneGroupOut_spectral]]
    #    grouping_column = SpectralClustering_14Clusters

[Models]
	#[[LinearRegression]]
	#[[KernelRidge]]
    #	alpha = 0.009
	#	gamma = 0.027
	#	kernel = rbf
	#[[KernelRidge_select]]
    #	alpha = 0.009
	#	gamma = 0.027
	#	kernel = rbf
	#[[KernelRidge_learning]]
    #	alpha = 0.009
	#	gamma = 0.027
	#	kernel = rbf
	#[[RandomForestRegressor]]
	 #   criterion = mse
	  #  max_depth = 10
	   # max_leaf_nodes = 200
	    #min_samples_leaf = 1
	  #  min_samples_split = 2
	   # n_estimators = 10
#	[[GaussianProcessRegressor]]
	#[[ExtraTreesRegressor_learning]]
	#[[RandomForestRegressor_select]]
	#    criterion = mse
	#    max_depth = 10
	#    max_leaf_nodes = 200
	#    min_samples_leaf = 1
	#    min_samples_split = 2
	#    n_estimators = 10
	[[MLPRegressor]]
        #hidden_layer_sizes = 50, 4
        hidden_layer_sizes = 296, 26
        activation = relu
        solver = adam
        alpha = 0.0001
       batch_size = auto
     learning_rate = constant

	#[[MLPClassifier]]
    #	hidden_layer_sizes = 100 
    #	activation = relu
    #	solver = adam
    #	alpha = 0.0001
    #   batch_size = auto
    #   learning_rate  = constant
    #   learning_rate_init =
    #   batch_size =
	
[PlotSettings]
    #feature_learning_curve = False
    #data_learning_curve = False
    target_histogram = True
    train_test_plots = True
    predicted_vs_true = True
    predicted_vs_true_bars = True
    best_worst_per_point = True
    feature_vs_target = False
    average_normalized_errors = True
